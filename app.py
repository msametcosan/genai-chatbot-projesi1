# --- 1. AdÄ±m: Gerekli ModÃ¼lleri Import Etme ---

# Ã–nbellek (Cache) hatasÄ±nÄ± Ã§Ã¶zmek iÃ§in bu bÃ¶lÃ¼m en Ã¼ste eklendi
import os
cache_dir = "/tmp/sentence_transformers_cache/"
os.makedirs(cache_dir, exist_ok=True)
os.environ["SENTENCE_TRANSFORMERS_HOME"] = cache_dir

import streamlit as st
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
import wikipedia
import time

# --- 2. AdÄ±m: Sayfa YapÄ±landÄ±rmasÄ± ve BaÅŸlÄ±k ---
st.set_page_config(
    page_title="Akbank GenAI Bootcamp RAG Chatbot",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– Dinamik Bilgi KaynaklÄ± RAG Chatbot")
# DÃœZELTME: Modeli 'Gemini Pro' olarak gÃ¼ncelledik
st.caption(
    f"Akbank GenAI Bootcamp Projesi - Wikipedia'dan alÄ±nan verilerle Gemini Pro ve FAISS kullanÄ±larak oluÅŸturulmuÅŸtur."
)

# --- 3. AdÄ±m: API AnahtarÄ± YapÄ±landÄ±rmasÄ± (Secrets'tan Okuma) ---
try:
    GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
    if not GOOGLE_API_KEY:
        GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY")
    if not GOOGLE_API_KEY:
        st.error(
            "âŒ GOOGLE_API_KEY bulunamadÄ±. LÃ¼tfen Hugging Face Spaces 'Secrets' bÃ¶lÃ¼mÃ¼ne ekleyin.")
        st.stop()
    genai.configure(api_key=GOOGLE_API_KEY)
except Exception as e:
    st.error(f"âŒ API anahtarÄ± yapÄ±landÄ±rÄ±lÄ±rken bir hata oluÅŸtu: {e}")
    st.stop()

# --- 4. AdÄ±m: Modelleri YÃ¼kleme (Cache ve HÄ±z Optimizasyonu ile) ---
@st.cache_resource
def load_models():
    """Embedding ve Generative modelleri bir kez yÃ¼kler."""
    
    # Cache dizinini, fonksiyonun iÃ§inde de aÃ§Ä±kÃ§a tanÄ±mla
    cache_dir = "/tmp/sentence_transformers_cache/"
        
    with st.spinner("ğŸ§  Yapay zeka modelleri yÃ¼kleniyor... (Bu iÅŸlem yalnÄ±zca ilk aÃ§Ä±lÄ±ÅŸta biraz zaman alabilir)"):
        try:
            # DÃœZELTME: 'cache_folder' parametresi fonksiyonun Ä°Ã‡Ä°NE eklendi
            embedding_model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                cache_folder=cache_dir 
            )
            
            # DÃœZELTME: Yetkimiz olan 'gemini-pro-latest' modeline geri dÃ¶nÃ¼ldÃ¼
            gen_model = genai.GenerativeModel('gemini-pro-latest')
            
            return embedding_model, gen_model
        
        except Exception as e:
            st.error(
                f"âŒ Modeller yÃ¼klenirken bir hata oluÅŸtu: {e}. LÃ¼tfen internet baÄŸlantÄ±nÄ±zÄ± kontrol edin veya sayfayÄ± yenileyin.")
            st.stop()

# Modelleri yÃ¼kle
embedding_model, gen_model = load_models()


# --- 5. AdÄ±m: Bilgi KaynaÄŸÄ± OluÅŸturma Fonksiyonu ---
@st.cache_data(show_spinner=False)
def bilgi_kaynagi_olustur(konu):
    """
    Belirtilen konu hakkÄ±nda Wikipedia'dan bilgi alÄ±r, iÅŸler ve
    arama yapÄ±labilecek bir FAISS indeksi ile veri setini (DataFrame) dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        with st.status(f"ğŸ“š '{konu}' hakkÄ±nda bilgi kaynaÄŸÄ± oluÅŸturuluyor...", expanded=True) as status:
            status.write(f"Wikipedia'dan '{konu}' konusu aranÄ±yor...")
            wikipedia.set_lang("tr")
            arama_sonuclari = wikipedia.search(konu)
            if not arama_sonuclari:
                st.error(f"âŒ '{konu}' ile ilgili bir Wikipedia sayfasÄ± bulunamadÄ±.")
                status.update(label="âŒ Konu bulunamadÄ±.", state="error")
                return None, None, None

            sayfa_basligi = arama_sonuclari[0]
            page = wikipedia.page(sayfa_basligi, auto_suggest=False)
            status.write(f"ğŸ“„ '{page.title}' sayfasÄ± baÅŸarÄ±yla bulundu ve iÅŸleniyor.")
            
            chunks = []
            paragraflar = page.content.split('\n\n')
            for p in paragraflar:
                if len(p.strip()) > 100:
                    chunks.append({'source': page.title, 'text': p.strip()})
            
            if not chunks:
                st.error("âŒ Sayfa iÃ§eriÄŸi iÅŸlenecek kadar uzun paragraflara ayrÄ±lamadÄ±.")
                status.update(label="âŒ Sayfa iÃ§eriÄŸi yetersiz.", state="error")
                return None, None, None

            df_chunks = pd.DataFrame(chunks)
            status.write(f"ğŸ§© {len(df_chunks)} adet metin parÃ§asÄ± (chunk) vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
            
            embeddings = embedding_model.encode(df_chunks['text'].tolist(), show_progress_bar=False)
            df_chunks['embeddings'] = list(embeddings)
            
            embeddings_array = np.array(df_chunks['embeddings'].tolist()).astype('float32')
            d = embeddings_array.shape[1]
            index = faiss.IndexFlatL2(d)
            index.add(embeddings_array)
            
            status.write("âœ… FAISS arama indeksi baÅŸarÄ±yla oluÅŸturuldu.")
            time.sleep(1)
            status.update(label=f"âœ… '{page.title}' konusu sohbete hazÄ±r!", state="complete")
            
            return df_chunks, index, page.title

    except wikipedia.exceptions.PageError:
        st.error(f"âŒ '{konu}' adÄ±nda bir Wikipedia sayfasÄ± bulunamadÄ±.")
        return None, None, None
    except wikipedia.exceptions.DisambiguationError as e:
        st.error(f"âŒ '{konu}' birden Ã§ok anlama geliyor. LÃ¼tfen daha spesifik olun. (Ã–rn: {e.options[:3]})")
        return None, None, None
    except Exception as e:
        st.error(f"âŒ Bilgi kaynaÄŸÄ± oluÅŸturulurken beklenmedik bir hata oluÅŸtu: {e}")
        return None, None, None


# --- 6. AdÄ±m: Soru Cevaplama Fonksiyonu (RAG Pipeline) ---
def soru_cevapla(soru, df_chunks, index):
    """
    KullanÄ±cÄ± sorusunu ve oluÅŸturulan bilgi kaynaÄŸÄ±nÄ± (index) kullanarak cevap Ã¼retir.
    """
    try:
        soru_vector = embedding_model.encode([soru]).astype('float32')
        k = 5
        distances, indices = index.search(soru_vector, k)
        relevant_chunks = [df_chunks.iloc[i]['text'] for i in indices[0]]
        context = "\n\n".join(relevant_chunks)
        
        prompt = f"""AÅŸaÄŸÄ±daki BÄ°LGÄ°LER bÃ¶lÃ¼mÃ¼nde verilen metinleri kullanarak SORU'yu cevapla.
CevabÄ±nÄ± yalnÄ±zca ve yalnÄ±zca sana verilen bu BÄ°LGÄ°LER'e dayandÄ±r. EÄŸer bilgiler soruyu cevaplamak iÃ§in yetersizse, 'Bu konuda bilgim yok.' de.
BÄ°LGÄ°LER:
{context}
SORU: {soru}
CEVAP:"""

        response = gen_model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"âŒ Cevap Ã¼retilirken bir hata oluÅŸtu: {e}")
        return "ÃœzgÃ¼nÃ¼m, cevap Ã¼retirken teknik bir sorunla karÅŸÄ±laÅŸtÄ±m."


# --- 7. AdÄ±m: Ana Uygulama ArayÃ¼zÃ¼ ve Sohbet MantÄ±ÄŸÄ± ---

if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_topic" not in st.session_state:
    st.session_state.current_topic = ""
if "current_index" not in st.session_state:
    st.session_state.current_index = None
if "current_df" not in st.session_state:
    st.session_state.current_df = None

# --- Yan MenÃ¼ (Sidebar) ---
with st.sidebar:
    st.header("Konu SeÃ§imi")
    st.markdown("""
    Chatbot'un hangi Wikipedia konusu hakkÄ±nda konuÅŸacaÄŸÄ±nÄ± buradan belirleyebilirsiniz.
    """)
    
    yeni_konu = st.text_input("Wikipedia Konusu Girin:", placeholder="Ã–rn: Yapay zeka")
    
    if st.button("Yeni Konuyu Ayarla", type="primary"):
        if yeni_konu:
            st.session_state.current_topic = yeni_konu
            st.session_state.messages = []
            
            df, index, title = bilgi_kaynagi_olustur(yeni_konu)
            
            if df is not None and index is not None:
                st.session_state.current_df = df
                st.session_state.current_index = index
                st.session_state.current_topic = title
            else:
                st.session_state.current_topic = ""
                st.session_state.current_df = None
                st.session_state.current_index = None
        else:
            st.sidebar.warning("LÃ¼tfen bir konu adÄ± girin.")
            
    st.divider()
    st.markdown("Proje dÃ¶kÃ¼manÄ±na [buradan](https://github.com/muratdrd/akbanka1) ulaÅŸabilirsiniz.") # Buradaki linki kendi GitHub reponuzla deÄŸiÅŸtirmeyi unutmayÄ±n

# --- Ana Sohbet AlanÄ± ---
if not st.session_state.current_topic:
    st.info("LÃ¼tfen sohbete baÅŸlamak iÃ§in kenar Ã§ubuÄŸundan bir Wikipedia konusu belirleyin.")
    st.stop()

st.info(f"Åu anki sohbet konusu: **{st.session_state.current_topic}** (Wikipedia'dan alÄ±ndÄ±)")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input(f"'{st.session_state.current_topic}' hakkÄ±nda bir soru sorun..."):
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Cevap oluÅŸturuluyor..."):
            cevap = soru_cevapla(prompt, st.session_state.current_df, st.session_state.current_index)
            st.markdown(cevap)
    
    st.session_state.messages.append({"role": "assistant", "content": cevap})

